{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fbb9860-59e7-4e53-a091-a5b9b010ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf578bb-52d7-493a-b143-3611d9f82c43",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e90ab1-c8f7-4117-a7b4-4972abb1a4d3",
   "metadata": {},
   "source": [
    "## load dataset\n",
    "\n",
    "* unstructure data - notes \n",
    "    * extract cheif concern and history info.\n",
    " \n",
    "* structure data - lab and chart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c5d5761-a642-4e22-8328-6443a9f5f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_notes = pd.read_json('./sepsis_sample_notes.json', lines=True)\n",
    "df_sample_lab = pd.read_csv('./sepsis_sample_lab.csv')\n",
    "df_sample_diag = pd.read_csv('./sepsis_sample_diag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "fb3e0a65-e1de-4e9f-bdcb-259766a637dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "long_title\n",
       "Sepsis                                                                                                     71\n",
       "Pneumonia, unspecified organism                                                                            47\n",
       "Severe sepsis with septic shock                                                                            36\n",
       "Cardiogenic shock                                                                                          11\n",
       "Systemic inflammatory response syndrome (SIRS) of non-infectious origin without acute organ dysfunction     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check distribution\n",
    "df_sample_diag['long_title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eafcd3-09cf-40e7-bba9-1d5983926d44",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "* extract key info from note. (regexp)\n",
    "* get patient lab + chartevent\n",
    "* concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "84ffc9f8-f073-4eaf-bcb1-d1a32f9e37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing notes \n",
    "#use regexp to extract key info. (primary care)\n",
    "#print(df_sample_notes['text'][1])\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_pre_physical_exam(text):\n",
    "    \"\"\"\n",
    "    Extracts all text before the 'Physical Exam' section.\n",
    "    \"\"\"\n",
    "    match = re.search(r'^(.*?)\\n\\s*Physical Exam:', text, re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "#print(df_sample_notes['text'].apply(extract_pre_physical_exam)[2])\n",
    "df_sample_notes['extract_notes'] = df_sample_notes['text'].apply(extract_pre_physical_exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7aa7b7-ae1e-4fe7-bab3-89619597b127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#processing lab and chart result. \n",
    "\n",
    "df_sample_lab.head(5) #aggregate according to hadm_id\n",
    "#df_sample_lab_lab = df_sample_lab[df_sample_lab['type']==\"lab\"]\n",
    "#df_sample_lab_chart = df_sample_lab[df_sample_lab['type']==\"icu_chart\"]\n",
    "\n",
    "#df_sample_lab['type'].value_counts()\n",
    "\n",
    "\n",
    "def group_hadm_item_js(df):\n",
    "\n",
    "    # Convert 'charttime' to datetime for sorting\n",
    "    df['charttime'] = pd.to_datetime(df['charttime'])\n",
    "    \n",
    "    # Group by 'hadm_id' and aggregate into nested dictionary format\n",
    "    def aggregate_lab_results(group):\n",
    "        grouped_data = {}\n",
    "        for _, row in group.iterrows():\n",
    "            time_str = row['charttime'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            if time_str not in grouped_data:\n",
    "                grouped_data[time_str] = {}\n",
    "            grouped_data[time_str][row['label']] = f\"{row['value']} - {row['flag']}\" if pd.notna(row['flag']) else row['value']\n",
    "        return grouped_data\n",
    "    \n",
    "    # Apply grouping and sorting\n",
    "    grouped_df = df.sort_values(by=['charttime']).groupby('hadm_id').apply(aggregate_lab_results).reset_index()\n",
    "    grouped_df.columns = ['hadm_id', 'item_result']  # Rename columns\n",
    "    #grouped_df.to_excel('./grouped_df.xlsx')\n",
    "    return grouped_df\n",
    "\n",
    "#hadm, json.\n",
    "df_sample_lab_lab = group_hadm_item_js(df_sample_lab[df_sample_lab['type']==\"lab\"])\n",
    "df_sample_lab_chart = group_hadm_item_js(df_sample_lab[df_sample_lab['type']==\"icu_chart\"])\n",
    "\n",
    "\n",
    "df_sample_lab_lab[\"item_result\"] = df_sample_lab_lab[\"item_result\"].apply(json.dumps)\n",
    "df_sample_lab_chart[\"item_result\"] = df_sample_lab_chart[\"item_result\"].apply(json.dumps)\n",
    "\n",
    "\n",
    "df_sample_lab_lab.head(1)\n",
    "df_sample_lab_chart.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cc1f9-ba46-4ad3-92c1-a436f3fe72a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#construct a table , inner join full information. \n",
    "\n",
    "df_sample_diag_gp = df_sample_diag.groupby('hadm_id').agg({'long_title': list}).reset_index(drop = False)\n",
    "df_sample_diag_gp.head(1)\n",
    "\n",
    "\n",
    "df_sample_result = (df_sample_diag_gp.merge(df_sample_notes[['hadm_id','extract_notes']], how = 'inner', on = 'hadm_id')\n",
    ".merge(df_sample_lab_lab, how = 'inner', on = 'hadm_id')\n",
    ".merge(df_sample_lab_chart, how = 'inner', on = 'hadm_id'))\n",
    "\n",
    "df_sample_result['patient_info'] = df_sample_result['extract_notes'] + df_sample_result['item_result_x'] + df_sample_result['item_result_y']\n",
    "df_sample_result.head(1)\n",
    "#result_x is lab \n",
    "#result_y is chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b78333-710f-4c6b-b091-180257edb4e8",
   "metadata": {},
   "source": [
    "# Query LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0532ba-2ed5-4318-b78c-f20e4987aa94",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "09aa6ca5-def1-48e8-9047-90fb6e215b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat together. \n",
    "\n",
    "concat_diagnosis_prompt = \"\"\"\n",
    "You are a sepsis diagnosis expert. Please assess the following patient's data and determine if they have sepsis and its severity. Follow this structured diagnostic reasoning:\n",
    "\n",
    "Step 1: Sepsis Suspected?\n",
    "Check if the patient meets Systemic Inflammatory Response Syndrome (SIRS) criteria:\n",
    "\n",
    "- Temperature <36°C (96.8°F) OR >38°C (100.4°F)\n",
    "- Heart Rate (HR) >90 bpm\n",
    "- Respiratory Rate (RR) >20 OR pCO₂ <32 mmHg\n",
    "- White Blood Cell (WBC) Count >12,000/µL OR <4,000/µL OR >10% Bands\n",
    "- Sepsis is suspected if at least 2 of the above criteria are met AND there is a confirmed or suspected infection.\n",
    "\n",
    "Step 2: Severe Sepsis?\n",
    "If sepsis is suspected, check for organ dysfunction (indicating severe sepsis). The patient has severe sepsis if any of the following are present:\n",
    "\n",
    "- Neurological: Altered Mental Status\n",
    "- Circulatory: Systolic Blood Pressure (SBP) <90 mmHg OR Mean Arterial Pressure (MAP) <65 mmHg\n",
    "- Renal: Creatinine (Cr) >2 mg/dL OR increase of >0.5 mg/dL from baseline\n",
    "- Liver: Bilirubin >2 mg/dL\n",
    "- Coagulation: INR >1.5 OR PTT >60 seconds OR Platelets <100,000/uL\n",
    "- Urine Output: <0.5 mL/kg/hr for >2 hours\n",
    "- Lactate: Above upper normal limits (e.g., >2 mmol/L)\n",
    "- Respiratory: Acute respiratory failure requiring invasive/non-invasive ventilation\n",
    "\n",
    "Step 3: Septic Shock?\n",
    "If the patient has severe sepsis, check if they meet septic shock criteria:\n",
    "\n",
    "- SBP <90 mmHg OR MAP <65 mmHg despite adequate fluid resuscitation\n",
    "- Lactate ≥4 mmol/L, regardless of timing of fluid administration.\n",
    "\n",
    "Finally, provide a structured assessment like {\"Diagnosis\":\"No Sepsis\", \"Reason\":\"Your analysis\".}\n",
    "\n",
    "\n",
    "If sepsis is diagnosed, classify it as:\n",
    "- No Sepsis (SIRS <2 or no infection)\n",
    "- Sepsis (SIRS ≥2 + suspected infection)\n",
    "- Severe Sepsis (Sepsis + organ dysfunction)\n",
    "- Septic Shock (Severe sepsis + circulatory failure)\n",
    "\n",
    "Here is the patient infomartion: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5cec6-0bf1-412c-8925-a0c1b59670f8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4717602-1384-4631-81cd-e4f7c2da92b2",
   "metadata": {},
   "source": [
    "### Deepseek - r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "1aa09f9b-0953-426d-8a5c-964fb7452767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def request_llm(instruct, info):\n",
    "    \n",
    "    client = OpenAI(api_key=Deepseek_API_KEY, base_url=\"https://api.deepseek.com/v1\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-reasoner\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": instruct},\n",
    "                {\"role\": \"user\", \"content\": info},\n",
    "            ],\n",
    "            stream=False\n",
    "            #json_True\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#result = query_llm_ds(concat_diagnosis_prompt, patient_info )\n",
    "\n",
    "#print(result.choices[0].message.content)\n",
    "\n",
    "#request_llm(concat_diagnosis_prompt, df_sample_result.iloc[0,-1]) #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "94bf2795-9dce-42ae-b1a5-c0abbbf9ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmac\n",
    "import urllib\n",
    "import requests\n",
    "import arrow\n",
    "import base64\n",
    "import string\n",
    "import random\n",
    "import hashlib\n",
    "import json\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "import time\n",
    "\n",
    "def update_df_with_api_response(row_index, text, prompt):\n",
    "    response = request_llm(prompt, text)\n",
    "    return row_index, response\n",
    "\n",
    "\n",
    "def dataframe_request(df, text_col, res_col, prompt, num_threads=3):\n",
    "    \"\"\"\n",
    "    Process the DataFrame to make API requests for non-empty, non-None entries in a specified column.\n",
    "\n",
    "    Params:\n",
    "    - df (pd.DataFrame): The DataFrame to process.\n",
    "    - text_col (str): The column from which text needs to be analyzed.\n",
    "    - prompt (str): System prompt for the API request.\n",
    "    - num_threads (int, optional): Number of concurrent threads to use, default is 3.\n",
    "    \"\"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for index, row in df.iterrows():\n",
    "            # Check if the text column is empty, NaN or None, skip these rows\n",
    "            if pd.isna(row[text_col]) or row[text_col] == '' or row[text_col] is None:\n",
    "                continue\n",
    "            # Check if api_response is empty or NaN, then initiate an API call\n",
    "            if pd.isna(row[res_col]) or row[res_col] == '':\n",
    "                futures.append(executor.submit(update_df_with_api_response, index, row[text_col], prompt))\n",
    "\n",
    "        # Wait for all thread tasks to complete and collect results\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            row_index, response = future.result()\n",
    "            # Update DataFrame\n",
    "            df.at[row_index, res_col] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "fcccff51-bbc7-4627-b14d-511e16c26ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_result['result_deepseek'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "fba07b95-15bb-4a59-9ee0-df5be8f76425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch request\n",
    "dataframe_request(df_sample_result, 'patient_info','result_deepseek', concat_diagnosis_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "a441db08-9c6d-40bd-bc0a-e2f3113a23cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample_result.to_excel('./df_sample_result_deepseek.xlsx')\n",
    "#similar_result\n",
    "df_sample_result_ds= df_sample_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5155b-a338-4df8-9cc4-96a8a5cdeb28",
   "metadata": {},
   "source": [
    "### ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "2b15b2d3-1456-4e78-970b-a862baca0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI  \n",
    "\n",
    "\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://chatgpt-simulation.openai.azure.com/\")  \n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o-2\")  \n",
    "\n",
    "def request_llm(instruct, info):\n",
    "\n",
    "    client = AzureOpenAI(  \n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version=\"2024-05-01-preview\",)\n",
    "\n",
    "    try:\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": instruct},\n",
    "                {\"role\": \"user\", \"content\": info},\n",
    "            ]\n",
    "        completion = client.chat.completions.create(  \n",
    "        model=deployment,\n",
    "        messages=messages,\n",
    "        max_tokens=800,  \n",
    "        temperature=0.7,  \n",
    "        top_p=0.95,  \n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0,\n",
    "        stop=None,  \n",
    "        stream=False\n",
    "    )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "d7adce2d-24de-4a54-a4bf-2cd6ea4db330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_result['result_gpt'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "98db8bbf-08a1-45c2-b8cb-e2ac020fd278",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[397], line 44\u001b[0m, in \u001b[0;36mdataframe_request\u001b[0;34m(df, text_col, res_col, prompt, num_threads)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Wait for all thread tasks to complete and collect results\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(futures):\n\u001b[1;32m     45\u001b[0m     row_index, response \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:243\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    241\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 243\u001b[0m waiter\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mwait(wait_timeout)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    356\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[570], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataframe_request(df_sample_result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_info\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult_gpt\u001b[39m\u001b[38;5;124m'\u001b[39m, concat_diagnosis_prompt)\n",
      "Cell \u001b[0;32mIn[397], line 33\u001b[0m, in \u001b[0;36mdataframe_request\u001b[0;34m(df, text_col, res_col, prompt, num_threads)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataframe_request\u001b[39m(df, text_col, res_col, prompt, num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    Process the DataFrame to make API requests for non-empty, non-None entries in a specified column.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    - num_threads (int, optional): Number of concurrent threads to use, default is 3.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mnum_threads) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     34\u001b[0m         futures \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;66;03m# Check if the text column is empty, NaN or None, skip these rows\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshutdown(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/thread.py:238\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 238\u001b[0m         t\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:1149\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:1169\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1170\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#batch request\n",
    "dataframe_request(df_sample_result, 'patient_info','result_gpt', concat_diagnosis_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "76bfe12e-be08-4db7-9f00-db1b32c93824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_sample_result.to_excel('./df_sample_result_gpt.xlsx')\n",
    "#df_sample_result\n",
    "df_sample_result_gpt = df_sample_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a2f2a-b3c0-4843-ba48-9711681becc4",
   "metadata": {},
   "source": [
    "# Result Analysis\n",
    "\n",
    "try to extract key value from dirty json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "210744ca-ff6b-4c22-bcd4-5727b0c01488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# Function to clean JSON strings, if needed\n",
    "def clean_json(json_str):\n",
    "    # Check if the JSON string is not null\n",
    "    if pd.notna(json_str):\n",
    "        # Find the first occurrence of a valid JSON structure\n",
    "        match = re.search(r'{.*}', json_str, re.DOTALL)\n",
    "        if match:\n",
    "            cleaned_json_str = match.group(0)\n",
    "            # Attempt to load the JSON to see if it's valid\n",
    "            try:\n",
    "                # This also helps in removing unnecessary whitespaces\n",
    "                cleaned_json = json.loads(cleaned_json_str)\n",
    "                # Convert it back to a string to ensure consistency\n",
    "                return json.dumps(cleaned_json)\n",
    "            except json.JSONDecodeError:\n",
    "                pass  # If JSON is invalid, you might want to handle this case.\n",
    "    return ''\n",
    "\n",
    "\n",
    "# Extract key from JSON string\n",
    "def extract_category_type(json_str, entity):\n",
    "    try:\n",
    "        # Load the JSON string into a dictionary\n",
    "        data = json.loads(json_str)\n",
    "        # Extract the value for the specified entity\n",
    "        return data.get(entity, None)\n",
    "    except json.JSONDecodeError:\n",
    "        # Return None if there is an error decoding the JSON\n",
    "        return None\n",
    "\n",
    "# Extract keys from DataFrame column containing JSON strings\n",
    "def df_extract_category_type(entity_list, df, extracted_col):\n",
    "    # Ensure the JSON strings are clean and proper; this step is optional based on your data quality\n",
    "    df[extracted_col] = df[extracted_col].apply(clean_json)\n",
    "    \n",
    "    # For each entity in the list, create a new column in the DataFrame based on the extracted data\n",
    "    for entity in entity_list:\n",
    "        # Apply the extraction function across the DataFrame column\n",
    "        df[entity] = df[extracted_col].apply(lambda x: extract_category_type(x, entity))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac191b-77a7-4ca4-a562-c99ea8da03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = [\"Reason\", \"Diagnosis\"]\n",
    "#df_sample_result.head(1)\n",
    "\n",
    "df1 = df_extract_category_type(entity_list, df_sample_result, 'result_deepseek')\n",
    "#df1.to_excel('./result_deepseek_extract.xlsx')\n",
    "\n",
    "df2 = df_extract_category_type(entity_list, df_sample_result, 'result_gpt')\n",
    "#df2.to_excel('./result_gpt_extract.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "90b7d828-2430-483f-9824-a0c4690de9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
